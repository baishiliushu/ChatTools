import asyncio
import os
import json
import sys
import time

import aiohttp
from typing import Optional, List, Dict, Any
from contextlib import AsyncExitStack

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.client.sse import sse_client
from mcp.client.streamable_http import streamablehttp_client as shttp_client
from dotenv import load_dotenv
import logging
import httpx
from urllib.parse import urlparse
import datetime
import time

# Load .env
load_dotenv()

# åˆ›å»ºè‡ªå®šä¹‰æ ¼å¼åŒ–ç±»
class CustomFormatter(logging.Formatter):
    def formatTime(self, record, datefmt=None):
        # æ ¼å¼åŒ–æ—¶é—´ä¸º "æœˆ-æ—¥ æ—¶:åˆ†:ç§’.æ¯«ç§’"
        ct = self.converter(record.created)
        t = time.strftime("%m-%d %H:%M:%S", ct)
        s = "%s.%03d" % (t, record.msecs)
        return s

# é…ç½®æ—¥å¿—
def setup_logging():
    # åˆ›å»º logger
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # å¦‚æœå·²æœ‰å¤„ç†å™¨å­˜åœ¨ï¼Œå…ˆæ¸…é™¤
    if logger.hasHandlers():
        logger.handlers.clear()
    
    # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    
    # åˆ›å»ºæ ¼å¼åŒ–å™¨å¹¶ä½¿ç”¨è‡ªå®šä¹‰æ—¶é—´æ ¼å¼
    formatter = CustomFormatter('%(levelname)s %(asctime)s (%(message)s')
    
    # å°†æ ¼å¼åŒ–å™¨æ·»åŠ åˆ°å¤„ç†å™¨
    ch.setFormatter(formatter)
    
    # å°†å¤„ç†å™¨æ·»åŠ åˆ° logger
    logger.addHandler(ch)

# è®¾ç½®æ—¥å¿—
setup_logging()

# è·å– logger
logger = logging.getLogger("terminal_client")


class LocalMCPClient:
    def __init__(self, prompt_file_path = "prompt.txt"):
        self.exit_stack = AsyncExitStack()
        self.vllm_api_url = os.getenv("VLLM_API_URL", "http://192.168.50.208:8000/v1/chat/completions")
        self.model_name = os.getenv("MODEL_NAME", "QwQ-32B-AWQ")
        self.vllm_api_key = os.getenv("VLLM_API_KEY", "mindo")
        with open(prompt_file_path, "r", encoding="utf-8") as f:
            self.file_content = f.read()

        self.mcp_session: Optional[ClientSession] = None
        self.mcp_tools: list = []
        self.http_session: Optional[aiohttp.ClientSession] = None
        self.chat_history: List[Dict[str, Any]] = []
        self.test_queries = [
            "è®¾å¤‡indemind, ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
            "è®¾å¤‡indemind, è¯·ä½ å›åˆ°å……ç”µæ¡©å¾…å‘½",
            "è®¾å¤‡indemind, å¾€å‰èµ°ä¸¤ç±³",
            "è®¾å¤‡indemind, è¯·å¸®æˆ‘æ‰¾ä¸€ä¸‹å®¢å…çš„çŒ«",
            "è®¾å¤‡indemind, å»å§å®¤é è¿‘ä¸€ä¸‹åƒåœ¾æ¡¶",
            "è®¾å¤‡indemind, è¯·è¿›è¡Œä¸€æ¬¡å…¨å±‹çš„å·¡é€»æ£€æŸ¥",
            "è®¾å¤‡indemind, å…ˆå»ä¹¦æˆ¿æ‰¾ä¸€ä¸‹æ’æ’ï¼Œç„¶åå›åˆ°å……ç”µæ¡©",
            "è®¾å¤‡indemind, å…ˆå»å§å®¤æ‰¾ä¸€ä¸‹åƒåœ¾æ¡¶ï¼Œæ‰¾åˆ°åå†å»å®¢å…æ‰¾ç”µè§†ï¼Œç„¶åå›æ¡©"
        ]
        self.mcp_mode = os.getenv("MCP_MODE_BY_URL", "mcp_server.py")  # stdio, sse, shttp

    async def initialize_http_session(self):
        if not self.http_session:
            # é…ç½®è¿æ¥æ± å’Œè¶…æ—¶å‚æ•°
            connector = aiohttp.TCPConnector(
                limit=20,  # æœ€å¤§è¿æ¥æ•°
                limit_per_host=5,  # æ¯ä¸»æœºæœ€å¤§è¿æ¥æ•°
                ttl_dns_cache=300  # DNSç¼“å­˜æ—¶é—´
            )
            timeout = aiohttp.ClientTimeout(total=300)  # æ€»è¶…æ—¶
            
            # åˆ›å»ºå¸¦é…ç½®çš„ä¼šè¯
            self.http_session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout
            )
            
            try:
                # æ³¨å†Œåˆ°é€€å‡ºæ ˆ
                await self.exit_stack.enter_async_context(self.http_session)
            except Exception as e:
                # å¦‚æœæ³¨å†Œå¤±è´¥ï¼Œç¡®ä¿å…³é—­ä¼šè¯
                await self.http_session.close()
                self.http_session = None
                raise e


    def convert_mcp_tools_to_openai_format(self) -> List[Dict[str, Any]]:
        tools_openai_format = []
        for tool in self.mcp_tools:
            schema = tool.inputSchema if hasattr(tool, 'inputSchema') and isinstance(tool.inputSchema, dict) else {"type": "object", "properties": {}, "required": []}
            tools_openai_format.append({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": schema
                }
            })
        return tools_openai_format
    
    
    async def call_vllm_api(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None, stream_enable: bool = False) -> Optional[Dict[str, Any]]:
        if not self.http_session:
            await self.initialize_http_session()
        
        payload = {
            "model": self.model_name,
            "messages": messages,
            #"max_tokens": 90000,
            #"temperature": 0.5,
            "stream": stream_enable  # å¯ç”¨æµå¼è¾“å‡º
        }

        if tools:
            try:
                json.dumps(tools)  # Test serialization
                payload["tools"] = tools
                payload["tool_choice"] = "auto"
            except TypeError as e:
                logger.info(f"âŒ å·¥å…·åºåˆ—åŒ–å¤±è´¥: {e}")

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.vllm_api_key}"
        }

        logger.info(f"ğŸ”„ æ­£åœ¨å‘ vLLM å‘é€è¯·æ±‚ ({self.vllm_api_url})...{payload.keys()} \n{messages[0].keys()}")
        try:
            time_start_f = time.time()
            ret = {}
            async with self.http_session.post(self.vllm_api_url, headers=headers, json=payload, timeout=500) as response:
                if not stream_enable:
                    if response.status == 200:
                        logger.info(f"{type(response)}å“åº”0: {response} ")

                        result = await response.json()
                        logger.info(f"ğŸ” åŸå§‹ vLLM å“åº”:\n{json.dumps(result, indent=2, ensure_ascii=False)}\n")
                        time_end_f = time.time()
                        logger.info("è·å–åŸå§‹vLLM å“åº” time cost: {:.2f} s".format(time_end_f - time_start_f))
                        if result.get("choices") or result is not None:
                            choice = result["choices"][0]
                            ret["tool_calls"] = choice["message"]["tool_calls"]
                            ret["content"] = choice["message"]["content"]
                                                   
                        return ret
                    else:
                        error_text = await response.text()
                        logger.info(f"âŒ vLLM API è¯·æ±‚å¤±è´¥ã€‚çŠ¶æ€ç : {response.status}")
                        logger.info(f"å“åº”å†…å®¹: {error_text[:1000]}...")
                        return None
                else:
                    logger.info(f"{type(response)}å“åº”1: {response} ")
                    
                    # åˆå§‹åŒ–å˜é‡ç”¨äºç´¯ç§¯æµå¼æ•°æ®
                    current_tool_calls = {}
                    ret_content = ""
                    
                    async for line in response.content:
                        if line:
                            decoded_line = line.decode('utf-8').strip()
                            logger.info(f"Received data: {decoded_line}")
                            
                            if decoded_line.startswith('data: '):
                                # å¤„ç†SSEæ ¼å¼æ•°æ®
                                json_str = decoded_line[6:]  # ç§»é™¤"data: "å‰ç¼€
                                if json_str == "[DONE]":
                                    break
                                try:
                                    data = json.loads(json_str)
                                    if "choices" in data and len(data["choices"]) > 0:
                                        delta = data["choices"][0].get("delta", {})
                                        
                                        # å¤„ç†å†…å®¹æ›´æ–°
                                        if "content" in delta and delta["content"]:
                                            ret_content += delta["content"]
                                        
                                        # å¤„ç†å·¥å…·è°ƒç”¨æ›´æ–°
                                        if "tool_calls" in delta and delta["tool_calls"]:
                                            for tool_call_chunk in delta["tool_calls"]:
                                                index = tool_call_chunk.get("index", 0)
                                                
                                                # åˆå§‹åŒ–å½“å‰ç´¢å¼•çš„å·¥å…·è°ƒç”¨
                                                if index not in current_tool_calls:
                                                    current_tool_calls[index] = {
                                                        "id": "",
                                                        "type": "function",
                                                        "function": {
                                                            "name": "",
                                                            "arguments": ""
                                                        }
                                                    }
                                                
                                                # æ›´æ–°å·¥å…·è°ƒç”¨ä¿¡æ¯
                                                current_tool_call = current_tool_calls[index]
                                                
                                                if "id" in tool_call_chunk:
                                                    current_tool_call["id"] = tool_call_chunk["id"]
                                                
                                                if "type" in tool_call_chunk:
                                                    current_tool_call["type"] = tool_call_chunk["type"]
                                                
                                                if "function" in tool_call_chunk:
                                                    function_chunk = tool_call_chunk["function"]
                                                    
                                                    if "name" in function_chunk:
                                                        current_tool_call["function"]["name"] = function_chunk["name"]
                                                    
                                                    if "arguments" in function_chunk:
                                                        current_tool_call["function"]["arguments"] += function_chunk["arguments"]
                                    
                                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                                    finish_reason = data["choices"][0].get("finish_reason")
                                    if finish_reason == "tool_calls":
                                        # æ‰€æœ‰å·¥å…·è°ƒç”¨å·²å®Œæˆï¼Œå°è¯•è§£æå‚æ•°
                                        ret_tool_calls = []
                                        for index in sorted(current_tool_calls.keys()):
                                            tool_call = current_tool_calls[index]
                                            try:
                                                # å°è¯•è§£æå‚æ•°ä¸ºJSON
                                                if tool_call["function"]["arguments"]:
                                                    tool_call["function"]["arguments"] = tool_call["function"]["arguments"]
                                                ret_tool_calls.append(tool_call)
                                            except json.JSONDecodeError:
                                                # å¦‚æœè§£æå¤±è´¥ï¼Œä¿æŒåŸå§‹å­—ç¬¦ä¸²æ ¼å¼
                                                logger.warning(f"æ— æ³•è§£æå·¥å…·è°ƒç”¨å‚æ•°ä¸ºJSON: {tool_call['function']['arguments']}")
                                                ret_tool_calls.append(tool_call)
                                        
                                        ret["tool_calls"] = ret_tool_calls
                                        ret["content"] = ret_content
                                        time_end_f = time.time()
                                        logger.info("è·å–åŸå§‹vLLM å“åº” time cost: {:.2f} s".format(time_end_f - time_start_f))
                                        return ret
                                        
                                except json.JSONDecodeError:
                                    logger.info(f"æ— æ³•è§£æJSON: {json_str}")
                    
                    # å¦‚æœæµç»“æŸä½†æ²¡æœ‰æ˜ç¡®çš„å·¥å…·è°ƒç”¨å®Œæˆä¿¡å·ï¼Œä¹Ÿå°è¯•å¤„ç†å·²æ”¶é›†çš„æ•°æ®
                    ret_tool_calls = []
                    for index in sorted(current_tool_calls.keys()):
                        tool_call = current_tool_calls[index]
                        try:
                            if tool_call["function"]["arguments"]:
                                tool_call["function"]["arguments"] = tool_call["function"]["arguments"]
                            ret_tool_calls.append(tool_call)
                        except json.JSONDecodeError:
                            logger.warning(f"æ— æ³•è§£æå·¥å…·è°ƒç”¨å‚æ•°ä¸ºJSON: {tool_call['function']['arguments']}")
                            ret_tool_calls.append(tool_call)
                    
                    ret["tool_calls"] = ret_tool_calls
                    ret["content"] = ret_content
                    time_end_f = time.time()
                    logger.info("è·å–åŸå§‹vLLM å“åº” time cost: {:.2f} s".format(time_end_f - time_start_f))
                    return ret
                    

                    
                    # TODO: è§£æåˆ°å®Œæ•´toolååˆ¤æ–­
                    
                        # TODO: tool ä¸ä¸ºç©ºï¼Œæ”¾åˆ°retä¸­è¿”å›ï¼›toolä¸ºç©ºï¼Œç­‰å¾…contentæ‹¼æ¥å®Œåï¼Œæ”¾åˆ°retä¸­è¿”å›
                    

        except Exception as e:
            logger.info(f"âŒ è°ƒç”¨ vLLM API æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return None

    async def process_response(self, llm_response_data: Dict[str, Any], system_message: Dict[str, Any], query: str, stream_enable: bool = True) -> Optional[str]:
        """å¤„ç†APIå“åº”ï¼ˆæµå¼å’Œéæµå¼é€šç”¨ï¼‰"""
        if not llm_response_data or llm_response_data is None:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ€è€ƒæ—¶é‡åˆ°äº†ä¸€äº›éº»çƒ¦ã€‚"

        content = llm_response_data.get("content")
        tool_calls = llm_response_data.get("tool_calls")
        
        llm_response_data["reasoning_content"] = None
        llm_response_data["role"] = "assistant"
        logger.info(f"å“åº”åå¤„ç† -> å¤„ç†è¯·æ±‚ {llm_response_data}")

        if tool_calls:
            tool_name_contents = []
            tool_name_description = ""
            tool_content_description = ""
            
            for tool_call in tool_calls:
                tool_name = tool_call['function']['name']
                try:
                    tool_args = json.loads(tool_call['function']['arguments'])
                except json.JSONDecodeError:
                    logger.info(f"âŒ å·¥å…·å‚æ•°è§£æå¤±è´¥: {tool_call['function']['arguments']}")
                    continue
                
                logger.info(f"ğŸ› ï¸ çœŸå® ToolCall: {tool_name}, å‚æ•°: {tool_args}")
                mcp_result = await self.mcp_session.call_tool(tool_name, tool_args)
                tool_content = mcp_result.content[0].text if mcp_result.content else "å·¥å…·æœªè¿”å›ä»»ä½•å†…å®¹ã€‚"
                tool_name_contents.append({"tool_name": tool_name, "tool_content": tool_content})
            
            for tc in tool_name_contents:
                tool_name_description += " " + tc["tool_name"] + "  "
                tool_content_description += " " + tc["tool_content"] + "  "
            
            twice_input = f"å·¥å…·{tool_name_description}çš„è¯·æ±‚ç»“æœ:{tool_content_description}"
            if tool_content_description == "":
                ass = "è¯·æ±‚å·¥å…·ç»“æœå‡ºé”™ã€‚"
                self.chat_history.extend([{"role": "user", "content": query},{"role": "assistant", "content": ass}])
                return ass

            logger.info(f"ğŸ› ï¸ [-*-äºŒæ¬¡æ¨ç†è¾“å…¥-*-] {twice_input}")
            tool_result_msg = {
                "role": "user",
                "content": twice_input
            }

            self.chat_history.extend([{"role": "user", "content": query}, llm_response_data, tool_result_msg])
            
            # äºŒæ¬¡æ¨ç†ä¹Ÿä½¿ç”¨ç›¸åŒçš„éæµå¼æ¨¡å¼è®¾ç½®                
            final_response = await self.call_vllm_api([system_message] + self.chat_history, stream_enable=False)
            if final_response and final_response.get("content"):
                final_content = final_response.get("content")
            else:
                final_response = "æŠ±æ­‰ï¼Œå¤„ç†å·¥å…·ç»“æœæ—¶å‡ºé”™ã€‚"
            self.chat_history.append({"role": "assistant", "content": final_content})
            logger.info(f"\nğŸ¤– Assistant: {final_content}")
            return final_content
        else:
            self.chat_history.extend([{"role": "user", "content": query}, llm_response_data])
            return content

    async def process_query(self, query: str, stream_enable: bool = False) -> Optional[str]:
        
        if not self.http_session:
            await self.initialize_http_session()
        querrys = query.split("c+++")
        if query.startswith("c+++") or len(self.chat_history) > 60:
            #self.chat_history = []
            await self.clear_chat_history()
        query = querrys[-1]
        if len(query) < 1:
            return "EMPTY user words."
        logger.info(f"å‘é€è¯·æ±‚åˆ°æ¨¡å‹: {query}")
        system_message = {"role": "system", "content": self.file_content}
        messages = [system_message] + self.chat_history + [{"role": "user", "content": query}]
        tools_for_llm = self.convert_mcp_tools_to_openai_format()
        #print("openai mesages:\n{}".format(tools_for_llm))

        # æ ¹æ®æµå¼æ¨¡å¼é€‰æ‹©è°ƒç”¨æ–¹å¼
        if stream_enable:
            logger.info("ä½¿ç”¨æµå¼æ¨¡å¼")
            #llm_response_data = await self.call_vllm_api_streaming(messages, tools=tools_for_llm)
        else:
            logger.info("ä½¿ç”¨éæµå¼æ¨¡å¼")
        llm_response_data = await self.call_vllm_api(messages, tools=tools_for_llm, stream_enable=stream_enable)
        return  await self.process_response(llm_response_data, system_message, query, stream_enable)      

    async def clear_chat_history(self):
        l = len(self.chat_history)
        self.chat_history = []
        print("\nå·²æ¸…ç©ºï¼ˆ{}æ¡ï¼‰ä¼šè¯å†å²".format(l))
        return len(self.chat_history)
        
    async def chat_loop(self):
        logger.info("\nğŸ¤– æœ¬åœ° LLM + MCP å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡º")
        _stream_enable = False
        while True:
            try:
                query = input("\nä½ : ").strip()
                if not query:
                    continue
                if query.lower() == 'quit' or query.lower() == 'q':
                    break
                if query.lower() == 'ttt':
                    await self.run_batch_test()
                if query.lower() == 'stream on':
                    _stream_enable = True
                    logger.info("å·²å¯ç”¨æµå¼æ¨¡å¼")
                    continue
                if query.lower() == 'stream off':
                    _stream_enable = False
                    logger.info("å·²ç¦ç”¨æµå¼æ¨¡å¼")
                    continue
                if  query.lower() == 'c':
                    await self.clear_chat_history()
                else:
                    if _stream_enable:
                        print("ğŸ¤– Assistant: ", end="", flush=True)
                    response_text = await self.process_query(query, _stream_enable)
                    
            except KeyboardInterrupt:
                logger.info("\næ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
                break
            except Exception as e:
                logger.info(f"âš ï¸ å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")          


    async def run_batch_test(self):
        system_message = {"role": "system", "content": self.file_content}
        tools_for_llm = self.convert_mcp_tools_to_openai_format()

        stats = []

        for i, query in enumerate(self.test_queries):
            messages = [system_message, {"role": "user", "content": query}]
            logger.info(f"\nğŸ§ª æµ‹è¯• {i+1}/{len(self.test_queries)}: {query}")

            start = time.time()
            result = await self.call_vllm_api(messages, tools=tools_for_llm)
            end = time.time()

            duration = end - start
            stats.append((query, duration))
            logger.info(f"ğŸ•’ å“åº”è€—æ—¶: {duration:.2f} ç§’\n")

        logger.info("\nğŸ“Š æ‰¹é‡æµ‹è¯•å®Œæˆï¼\n")
        for query, duration in stats:
            logger.info(f"ã€{query}ã€‘è€—æ—¶: {duration:.2f} ç§’")

        all_times = [d for _, d in stats]
        logger.info(f"\nğŸ“ˆ å¹³å‡è€—æ—¶: {sum(all_times)/len(all_times):.2f} ç§’")
        logger.info(f"â±ï¸ æœ€å¿«: {min(all_times):.2f} ç§’ï¼Œæœ€æ…¢: {max(all_times):.2f} ç§’")


    async def cleanup(self):
        logger.info("æ­£åœ¨å…³é—­è¿æ¥...")
        await self.exit_stack.aclose()
        logger.info("è¿æ¥å·²å…³é—­ã€‚")

    
    async def _connect_stdio(self, server_script_path: str):
        """ä½¿ç”¨stdioæ¨¡å¼è¿æ¥"""
        if not os.path.isfile(server_script_path):
            raise FileNotFoundError(f"MCP Server è„šæœ¬æœªæ‰¾åˆ°: {server_script_path}")

        command = "python" if server_script_path.endswith('.py') else "node"
        server_params = StdioServerParameters(
            command=command,
            args=[server_script_path],
            env=os.environ.copy()
        )

        try:
            stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
            stdio_reader, stdio_writer = stdio_transport
            self.mcp_session = ClientSession(stdio_reader, stdio_writer)
            await self._initialize_session()
        except Exception as e:
            logger.info(f"âŒ ä½¿ç”¨ stdio æ¨¡å¼è¿æ¥åˆ° MCP Server å¤±è´¥: {e}")
            raise

    async def _connect_sse(self, sse_url: str):
        """ä½¿ç”¨SSEæ¨¡å¼è¿æ¥ urlé»˜è®¤ http://192.168.50.222:8087/sse"""
        try:
            os.environ.pop('ALL_PROXY', None)
            os.environ.pop('all_proxy', None)
                
            sse_transport = await self.exit_stack.enter_async_context(sse_client(sse_url))
            sse_reader, sse_writer = sse_transport
            self.mcp_session = ClientSession(sse_reader, sse_writer)
            await self._initialize_session()
        except Exception as e:
            logger.info(f"âŒ ä½¿ç”¨ SSE æ¨¡å¼è¿æ¥åˆ° MCP Server å¤±è´¥: {e} BY {sse_url}")
            raise

    async def _connect_shttp(self, shttp_url: str):
        """ä½¿ç”¨sHTTPæ¨¡å¼è¿æ¥"""
        try:
            os.environ.pop('ALL_PROXY', None)
            os.environ.pop('all_proxy', None)
            shttp_transport = await self.exit_stack.enter_async_context(shttp_client(shttp_url))
            shttp_reader = None
            shttp_writer = None
            shttp_get_session_id = None
            if len(shttp_transport) > 1:
                shttp_reader, shttp_writer, shttp_get_session_id = shttp_transport
                #if rest:
                    #print(f"[DDDD] Additional elements: {rest}")
                    #shttp_get_session_id = rest[0]
                    #exit(8)
            else:
                exit(-3)
            self.mcp_session = ClientSession(shttp_reader, shttp_writer)
            await self._initialize_session()
        except Exception as e:
            logger.info(f"âŒ ä½¿ç”¨ sHTTP æ¨¡å¼è¿æ¥åˆ° MCP Server å¤±è´¥: {e}")
            raise

    async def connect_to_mcp(self):
        """è¿æ¥åˆ°MCPæœåŠ¡å™¨ï¼Œæ”¯æŒä¸‰ç§æ¨¡å¼"""
        logger.info(f"å°è¯•ä½¿ç”¨ {self.mcp_mode.upper()} æ¨¡å¼è¿æ¥åˆ° MCP æœåŠ¡å™¨...")
        mcp_mode_by_url = self.mcp_mode
        if ".py" in  mcp_mode_by_url:
            await self._connect_stdio(mcp_mode_by_url)
        elif "sse" in  mcp_mode_by_url:
            await self._connect_sse(mcp_mode_by_url)
        elif "shttp" in  mcp_mode_by_url:
            await self._connect_shttp(mcp_mode_by_url)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ MCP æ¨¡å¼: {self.mcp_mode}")

    async def _initialize_session(self):
        """åˆå§‹åŒ–MCPä¼šè¯"""
        await self.exit_stack.enter_async_context(self.mcp_session)
        await self.mcp_session.initialize()
        response = await self.mcp_session.list_tools()
        self.mcp_tools = response.tools
        
        if not self.mcp_tools:
            logger.info("âš ï¸ MCP Server æœªæŠ¥å‘Šä»»ä½•å¯ç”¨å·¥å…·ã€‚")
        else:
            logger.info("âœ… å·²è¿æ¥åˆ° MCP Serverï¼Œæ”¯æŒä»¥ä¸‹å·¥å…·:", [tool for tool in self.mcp_tools])


async def main():
    if len(sys.argv) < 2:
        logger.info("Usage: python client.py <path_to_prompt>")
        sys.exit(1)

    prompt_txt_path = sys.argv[1]
    client = LocalMCPClient(prompt_txt_path)
    try:
        await client.initialize_http_session()
      
        await client.connect_to_mcp()
        await client.chat_loop()

    finally:
        await client.cleanup()


if __name__ == "__main__":
    asyncio.run(main())

